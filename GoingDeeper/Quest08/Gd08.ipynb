{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30a36513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770103e1",
   "metadata": {},
   "source": [
    "## 1. NSMC 데이터 분석 및 Huggingface dataset 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316f69d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b368922ca64abfb8cfe48aeb71889a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fe4741a3804f4d9d8333db0b511dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/807 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset nsmc/default (download: 18.62 MiB, generated: 20.90 MiB, post-processed: Unknown size, total: 39.52 MiB) to /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f72b4200cf4286bb716931b1642706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d06420269f43dd9ca3bbca188122e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/6.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b351e9d2ad4c28abfe355a6a25a1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.89M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c11afbf42d64bc7913451664e8f08dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset nsmc downloaded and prepared to /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1a7eb39a9e4835a3f93e3a306192a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 불러오기\n",
    "from datasets import load_dataset\n",
    "\n",
    "# NSMC 데이터셋 로드\n",
    "nsmc_dataset = load_dataset('nsmc')\n",
    "print(nsmc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c816e780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '9976970', 'label': 0, 'document': '아 더빙.. 진짜 짜증나네요 목소리'}\n"
     ]
    }
   ],
   "source": [
    "print(nsmc_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92318a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'document', 'label']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = nsmc_dataset['train']\n",
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e59ee684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 9976970\n",
      "document : 아 더빙.. 진짜 짜증나네요 목소리\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 3819312\n",
      "document : 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "label : 1\n",
      "\n",
      "\n",
      "id : 10265843\n",
      "document : 너무재밓었다그래서보는것을추천한다\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 9045019\n",
      "document : 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 6483659\n",
      "document : 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1d4070",
   "metadata": {},
   "source": [
    "## 2. klue/bert-base model 및 tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4052cc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f3589f100e412482154e413fcc94a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929aced02588487fb78c6eabd599f2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3860929cfa4dbe87e0a0a4f1dc9192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a379e5924f4969a11ac8c78d266bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e03a0eddd64ccda21e111787b0ec15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58a1918770449148024d9ae27cdb627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/424M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='klue/bert-base', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델과 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels=2)  # 데이터셋이 이진 분류이므로 2\n",
    "\n",
    "print(tokenizer)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23bfeb",
   "metadata": {},
   "source": [
    "## 3. 위에서 불러온 tokenizer으로 데이터셋을 전처리하고, model 학습 진행해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19b6f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'],  \n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,  \n",
    "        return_token_type_ids=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0316ee8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e9dc6dddeb448296497f24ea8a40e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7324652b984b44ec9b4c6204d474e17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋에 전처리 함수 적용\n",
    "nsmc_dataset = nsmc_dataset.map(transform, batched=True)\n",
    "\n",
    "# 학습을 위해 'train' 및 'test' 데이터셋을 나누기\n",
    "train_dataset = nsmc_dataset['train'].shuffle(seed=42).select(range(1000))  # 학습시간 줄이려고 샘플 작게 잡음\n",
    "test_dataset = nsmc_dataset['test'].shuffle(seed=42).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d508a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd48a8021414e1e90bd192b2bcfb0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "매핑 완료: 오류가 발생하지 않았습니다.\n"
     ]
    }
   ],
   "source": [
    "# 매핑 시 오류를 확인하기 위해 try-except 사용\n",
    "try:\n",
    "    # train 데이터셋에 transform 함수 매핑\n",
    "    tf_train_dataset = nsmc_dataset['train'].map(transform, batched=True)\n",
    "    print(\"매핑 완료: 오류가 발생하지 않았습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffd554f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ede10dc11d47b5af0769c5337e17b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 01:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>1.074975</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>1.209830</td>\n",
       "      <td>0.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.285302</td>\n",
       "      <td>0.815000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-125\n",
      "Configuration saved in ./results/checkpoint-125/config.json\n",
      "Model weights saved in ./results/checkpoint-125/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-250\n",
      "Configuration saved in ./results/checkpoint-250/config.json\n",
      "Model weights saved in ./results/checkpoint-250/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-375\n",
      "Configuration saved in ./results/checkpoint-375/config.json\n",
      "Model weights saved in ./results/checkpoint-375/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.09171615905314684, metrics={'train_runtime': 87.1884, 'train_samples_per_second': 34.408, 'train_steps_per_second': 4.301, 'total_flos': 197333291520000.0, 'train_loss': 0.09171615905314684, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# 평가 지표 불러오기\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 트레이너 초기화\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91350d5d",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning을 통하여 모델 성능(accuarcy) 향상시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9e7fb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 315\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 02:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>1.348969</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>1.599494</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.492635</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>1.412945</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>1.417326</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-63\n",
      "Configuration saved in ./results/checkpoint-63/config.json\n",
      "Model weights saved in ./results/checkpoint-63/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-126\n",
      "Configuration saved in ./results/checkpoint-126/config.json\n",
      "Model weights saved in ./results/checkpoint-126/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-189\n",
      "Configuration saved in ./results/checkpoint-189/config.json\n",
      "Model weights saved in ./results/checkpoint-189/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-252\n",
      "Configuration saved in ./results/checkpoint-252/config.json\n",
      "Model weights saved in ./results/checkpoint-252/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-315\n",
      "Configuration saved in ./results/checkpoint-315/config.json\n",
      "Model weights saved in ./results/checkpoint-315/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=315, training_loss=0.013433332489766477, metrics={'train_runtime': 166.3325, 'train_samples_per_second': 30.06, 'train_steps_per_second': 1.894, 'total_flos': 328888819200000.0, 'train_loss': 0.013433332489766477, 'epoch': 5.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TrainingArguments 수정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,  # 에포크 수 증가\n",
    "    per_device_train_batch_size=16,  # 배치 크기 증가\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=3e-5,  # 학습률 조정\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9a374f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-9c452e20bc4f5fcc.arrow\n",
      "Loading cached shuffled indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-92f2a57afd4479ec.arrow\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 크기 조정...\n",
    "train_dataset = nsmc_dataset['train'].shuffle(seed=42).select(range(10000))  \n",
    "test_dataset = nsmc_dataset['test'].shuffle(seed=42).select(range(2000)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0978cbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 19:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.659400</td>\n",
       "      <td>1.332021</td>\n",
       "      <td>0.841500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.179000</td>\n",
       "      <td>1.332021</td>\n",
       "      <td>0.841500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>1.332021</td>\n",
       "      <td>0.841500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.954400</td>\n",
       "      <td>1.332021</td>\n",
       "      <td>0.841500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.332021</td>\n",
       "      <td>0.841500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-625\n",
      "Configuration saved in ./results/checkpoint-625/config.json\n",
      "Model weights saved in ./results/checkpoint-625/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1250\n",
      "Configuration saved in ./results/checkpoint-1250/config.json\n",
      "Model weights saved in ./results/checkpoint-1250/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1875\n",
      "Configuration saved in ./results/checkpoint-1875/config.json\n",
      "Model weights saved in ./results/checkpoint-1875/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-3125\n",
      "Configuration saved in ./results/checkpoint-3125/config.json\n",
      "Model weights saved in ./results/checkpoint-3125/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3125, training_loss=1.234233298034668, metrics={'train_runtime': 1157.7422, 'train_samples_per_second': 43.188, 'train_steps_per_second': 2.699, 'total_flos': 3293097968885760.0, 'train_loss': 1.234233298034668, 'epoch': 5.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset = train_dataset\n",
    "trainer.eval_dataset = test_dataset\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "249c72d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3130\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1252' max='3130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1252/3130 14:32 < 21:50, 1.43 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.333600</td>\n",
       "      <td>0.342181</td>\n",
       "      <td>0.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.243600</td>\n",
       "      <td>0.364103</td>\n",
       "      <td>0.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.174800</td>\n",
       "      <td>0.383504</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>0.457265</td>\n",
       "      <td>0.876000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-313\n",
      "Configuration saved in ./results/checkpoint-313/config.json\n",
      "Model weights saved in ./results/checkpoint-313/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-626\n",
      "Configuration saved in ./results/checkpoint-626/config.json\n",
      "Model weights saved in ./results/checkpoint-626/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-939\n",
      "Configuration saved in ./results/checkpoint-939/config.json\n",
      "Model weights saved in ./results/checkpoint-939/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-1252\n",
      "Configuration saved in ./results/checkpoint-1252/config.json\n",
      "Model weights saved in ./results/checkpoint-1252/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-1252 (score: 0.876).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1252, training_loss=0.2354513618369072, metrics={'train_runtime': 873.0536, 'train_samples_per_second': 114.541, 'train_steps_per_second': 3.585, 'total_flos': 2631110553600000.0, 'train_loss': 0.2354513618369072, 'epoch': 4.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2차 대공사\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,  # 에포크 수 증가\n",
    "    per_device_train_batch_size=32,  # 배치 크기 증가\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,  # Best model 로드 옵션 추가\n",
    "    metric_for_best_model='accuracy',  # accuracy 기준 stop\n",
    "    learning_rate=1e-5,  # 더 낮은 학습률\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# EarlyStoppingCallback 추가\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # 개선되지 않은 에포크가 3회 발생하면 조기 종료\n",
    "    early_stopping_threshold=0.01  # 최소 개선 폭 설정 (선택 사항)\n",
    ")\n",
    "\n",
    "# Trainer 초기화 시 콜백 추가\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]  # Early Stopping 콜백 추가\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004f333",
   "metadata": {},
   "source": [
    "## 5. Bucketing을 적용하여 학습시키고, 4의 결과와 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5685ee38",
   "metadata": {},
   "source": [
    "1. Data Collator는 배치 단위로 데이터를 처리할 때 샘플을 모델 입력으로 변환하고 패딩을 추가하는 클래스임. 고정 길이 패딩이나 동적 패딩을 지원함. 동적 패딩은 각 배치에서 가장 긴 입력 시퀀스에 맞게 패딩하여 불필요한 메모리 사용을 줄이는 방법. -> transformers 라이브러리의 DataCollatorWithPadding 클래스를 사용하여 적용\n",
    "(참고 링크 : https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fbca6e",
   "metadata": {},
   "source": [
    "2. group_by_length는 모델 학습 시 비슷한 길이의 입력 시퀀스를 함께 배치로 묶는 Bucketing을 구현함.이 옵션을 사용하면 각 배치의 입력 길이 차이가 최소화되어, 각 배치에서 필요한 패딩의 양이 줄어들어 패딩 효율성을 높이고 학습 속도 개선, 메모리 절약 효과 있음\n",
    "(참고 링크 : https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "309b4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3130\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3130' max='3130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3130/3130 37:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.640007</td>\n",
       "      <td>0.866500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.826058</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.848997</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.976680</td>\n",
       "      <td>0.867500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>0.915006</td>\n",
       "      <td>0.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.905343</td>\n",
       "      <td>0.869500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.977087</td>\n",
       "      <td>0.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.949328</td>\n",
       "      <td>0.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.967539</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.972168</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-313\n",
      "Configuration saved in ./results/checkpoint-313/config.json\n",
      "Model weights saved in ./results/checkpoint-313/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-626\n",
      "Configuration saved in ./results/checkpoint-626/config.json\n",
      "Model weights saved in ./results/checkpoint-626/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-939\n",
      "Configuration saved in ./results/checkpoint-939/config.json\n",
      "Model weights saved in ./results/checkpoint-939/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-1252\n",
      "Configuration saved in ./results/checkpoint-1252/config.json\n",
      "Model weights saved in ./results/checkpoint-1252/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-1565\n",
      "Configuration saved in ./results/checkpoint-1565/config.json\n",
      "Model weights saved in ./results/checkpoint-1565/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-1878\n",
      "Configuration saved in ./results/checkpoint-1878/config.json\n",
      "Model weights saved in ./results/checkpoint-1878/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-2191\n",
      "Configuration saved in ./results/checkpoint-2191/config.json\n",
      "Model weights saved in ./results/checkpoint-2191/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-2504\n",
      "Configuration saved in ./results/checkpoint-2504/config.json\n",
      "Model weights saved in ./results/checkpoint-2504/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-2817\n",
      "Configuration saved in ./results/checkpoint-2817/config.json\n",
      "Model weights saved in ./results/checkpoint-2817/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-3130\n",
      "Configuration saved in ./results/checkpoint-3130/config.json\n",
      "Model weights saved in ./results/checkpoint-3130/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-2504 (score: 0.873).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3130, training_loss=0.028202080339514695, metrics={'train_runtime': 2224.9307, 'train_samples_per_second': 44.945, 'train_steps_per_second': 1.407, 'total_flos': 6577776384000000.0, 'train_loss': 0.028202080339514695, 'epoch': 10.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Collator with dynamic padding 설정\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# TrainingArguments 수정 (bucketing 적용)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,  # 에포크 수 증가\n",
    "    per_device_train_batch_size=32,  # 배치 크기 증가\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,  # Best model 로드 옵션 추가\n",
    "    metric_for_best_model='accuracy',  # EarlyStopping에서 사용할 지표\n",
    "    learning_rate=1e-5,  # 더 낮은 학습률 사용\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    group_by_length=True  # Bucketing 적용\n",
    ")\n",
    "\n",
    "# EarlyStoppingCallback 추가\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,  # 개선되지 않은 에포크가 3회 발생하면 조기 종료\n",
    ")\n",
    "\n",
    "# Trainer 초기화 시 data_collator 및 callback 추가\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,  # 동적 패딩 적용\n",
    "    callbacks=[early_stopping_callback]  # Early Stopping 콜백 추가\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80da5c",
   "metadata": {},
   "source": [
    "비교 : Bucketing 적용 여부에 따라 모델의 성능과 연산 효율성을 비교한 결과, Bucketing을 적용하지 않은 경우가 성능 면에서 더 효율적이었음. Bucketing을 적용하지 않았을 때 최종 Validation Accuracy는 0.876으로, Bucketing을 적용한 경우의 최종 Validation Accuracy인 0.870보다 약간 높았음. 또한, Validation Loss 역시 Bucketing을 적용하지 않은 경우 더 낮게 나타났음. 이는 Bucketing을 적용한 상태에서 모델이 학습 데이터에 과적합될 가능성을 시사함.\n",
    "\n",
    "Training Loss를 비교해 보면, Bucketing을 적용한 경우 더 낮은 값을 기록했지만, 이는 모델이 학습 데이터에 더 잘 맞춰졌음을 의미하며 과적합의 위험을 높일 수 있음. 반면, Bucketing을 적용하지 않은 경우에는 Early Stopping이 4번째 Epoch에서 작동하여 빠르게 학습을 마쳤고, Validation Accuracy도 더 높은 수준에서 유지되었음. Bucketing을 적용하지 않은 학습이 더 적은 Epoch 내에 안정적인 성능을 달성했다는 점에서 연산 효율성 측면에서도 이점이 있다고 할 수 있음.\n",
    "\n",
    "Bucketing의 주요 목적은 비슷한 길이의 시퀀스를 묶어 메모리 효율을 개선하고 학습 속도를 높이는 것임. 그러나 이번 실험에서는 학습 속도에서 눈에 띄는 개선이 없었으며, 이는 데이터셋의 시퀀스 길이가 이미 비슷해 Bucketing의 효과가 제한적이었을 가능성을 시사함.\n",
    "\n",
    "결론적으로, Bucketing은 모델 학습의 안정성과 메모리 효율성을 개선할 수 있는 방법이지만, 모든 데이터셋에 대해 성능을 향상시키는 것은 아님. 오히려 Bucketing을 적용하지 않은 경우 더 적은 Epoch 내에 효율적인 학습과 높은 성능을 달성할 수 있었음. 향후 Bucketing의 효과를 극대화하기 위해서는 더 다양한 시퀀스 길이를 가진 데이터셋이나 메모리 제약이 큰 환경에서 실험을 시도해보는 것이 좋겠음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc27ac8",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130fe2a",
   "metadata": {},
   "source": [
    "느낀점 : 샘플 수 적게 잡아서 시간 아끼려다가 파인튜닝에 고~대로 씀ㅎㅎ... 꾀부리지 말자....... \n",
    "배운점 : pre-trained 모델 사용법과 Dlthon 때 개념만 알고 사용하지 못했던 bucketing 사용해봐서 좋았음. \n",
    "아쉬운점 : accuracy 목표값 달성 못한 것. 파인튜닝을 잘 못하는듯... 성능을 개선시키려면 어떻게 해야하는지 아직 잘 모르겠다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
